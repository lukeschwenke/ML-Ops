{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Talha\\OneDrive\\Documents\\Code\\ML-Ops\\code\\modeling_dp.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/athletes.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/athletes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Talha\\OneDrive\\Documents\\Code\\ML-Ops\\code\\modeling_dp.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# There are Na values in some columns. Fill with 0\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtotal_lift\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcandj\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39msnatch\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mdeadlift\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mbacksq\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# There are Na values in some columns. Fill with 0\n",
    "df['total_lift'] = df['candj'].fillna(0) + df['snatch'].fillna(0) + df['deadlift'].fillna(0) + df['backsq'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vars = ['region','gender', 'eat', 'background', 'experience', 'schedule', 'howlong','age','height','weight','candj','snatch','deadlift','backsq']\n",
    "cats = ['region','gender', 'eat', 'background', 'experience', 'schedule', 'howlong']\n",
    "numcs = ['age','height','weight','candj','snatch','deadlift','backsq']\n",
    "\n",
    "# x = pd.get_dummies(x, columns=cats) # encode categorical variables\n",
    "\n",
    "x = df[numcs].fillna(0) # NAs in numeric columns, fill 0 if any\n",
    "\n",
    "y = df['total_lift']\n",
    "\n",
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
    "\n",
    "# model hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Define privacy parameters\n",
    "l2_norm_clip = 1.0  # Clipping norm\n",
    "noise_multiplier = 0.5  # Noise multiplier\n",
    "num_microbatches = 1  # Number of microbatches\n",
    "learning_rate = 0.1  # Learning rate\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(x_train.shape[1],)),   # Input layer for the number of features\n",
    "    tf.keras.layers.Dense(64, activation='relu'),       # Hidden layer 1\n",
    "    tf.keras.layers.Dense(32, activation='relu'),       # Hidden layer 2\n",
    "    tf.keras.layers.Dense(1)                            # Output layer (for regression)\n",
    "])\n",
    "\n",
    "# Create the optimizer with differential privacy\n",
    "optimizer = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)  # Use `dp_sum_query` for better privacy guarantees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1084, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer_v1.py\", line 874, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer_v1.py\", line 877, in apply_gradients\n        self.optimizer.apply_gradients(\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_privacy\\privacy\\optimizers\\dp_optimizer.py\", line 259, in apply_gradients\n        assert self._was_compute_gradients_called, (\n\n    AssertionError: compute_gradients() on the differentially private optimizer was not called. Which means that the training is not differentially private. It happens for example in Keras training in TensorFlow 2.0+.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Talha\\OneDrive\\Documents\\Code\\ML-Ops\\code\\modeling_dp.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileapqqqhjs.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_privacy\\privacy\\optimizers\\dp_optimizer.py:259\u001b[0m, in \u001b[0;36mmake_optimizer_class.<locals>.DPOptimizerClass.apply_gradients\u001b[1;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_gradients\u001b[39m(\u001b[39mself\u001b[39m, grads_and_vars, global_step\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    257\u001b[0m   \u001b[39m# pylint: disable=g-doc-args, g-doc-return-or-yield\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"DP-SGD version of base class method.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_was_compute_gradients_called, (\n\u001b[0;32m    260\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mcompute_gradients() on the differentially private optimizer was not\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    261\u001b[0m       \u001b[39m'\u001b[39m\u001b[39m called. Which means that the training is not differentially \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    262\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mprivate. It happens for example in Keras training in TensorFlow \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    263\u001b[0m       \u001b[39m'\u001b[39m\u001b[39m2.0+.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(DPOptimizerClass, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mapply_gradients(\n\u001b[0;32m    265\u001b[0m       grads_and_vars\u001b[39m=\u001b[39mgrads_and_vars, global_step\u001b[39m=\u001b[39mglobal_step, name\u001b[39m=\u001b[39mname)\n",
      "\u001b[1;31mAssertionError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1084, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer_v1.py\", line 874, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer_v1.py\", line 877, in apply_gradients\n        self.optimizer.apply_gradients(\n    File \"c:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_privacy\\privacy\\optimizers\\dp_optimizer.py\", line 259, in apply_gradients\n        assert self._was_compute_gradients_called, (\n\n    AssertionError: compute_gradients() on the differentially private optimizer was not called. Which means that the training is not differentially private. It happens for example in Keras training in TensorFlow 2.0+.\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 198169759.73359972\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss = model.evaluate(x_test, y_test)\n",
    "print(f'Test loss: {test_loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 7343.46\n",
      "Mean Squared Error (MSE): 198169753.58\n",
      "Root Mean Squared Error (RMSE): 14077.28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Compute regression metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)  # Calculate RMSE\n",
    "\n",
    "# Display the metrics summary\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute DP epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD performed over 24023 examples with 256 examples per iteration, noise\n",
      "multiplier 0.5 for 10 epochs with microbatching, and no bound on number of\n",
      "examples per user.\n",
      "\n",
      "This privacy guarantee protects the release of all model checkpoints in addition\n",
      "to the final model.\n",
      "\n",
      "Example-level DP with add-or-remove-one adjacency at delta = 1e-05 computed with\n",
      "RDP accounting:\n",
      "    Epsilon with each example occurring once per epoch:       138.688\n",
      "    Epsilon assuming Poisson sampling (*):                    139.347\n",
      "\n",
      "No user-level privacy guarantee is possible without a bound on the number of\n",
      "examples per user.\n",
      "\n",
      "(*) Poisson sampling is not usually done in training pipelines, but assuming\n",
      "that the data was randomly shuffled, it is believed the actual epsilon should be\n",
      "closer to this value than the conservative assumption of an arbitrary data\n",
      "order.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(compute_dp_sgd_privacy.compute_dp_sgd_privacy_statement(x_train.shape[0],\n",
    "                                    batch_size=256,\n",
    "                                    noise_multiplier=noise_multiplier,\n",
    "                                    num_epochs=epochs,\n",
    "                                    delta=.00001\n",
    "                                    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
