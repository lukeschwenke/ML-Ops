{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Talha\\OneDrive\\Documents\\Code\\ML-Ops\\code\\modeling_dp.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m../data/athletes.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1795\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1792\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1793\u001b[0m         new_rows \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(index)\n\u001b[1;32m-> 1795\u001b[0m     df \u001b[39m=\u001b[39m DataFrame(col_dict, columns\u001b[39m=\u001b[39;49mcolumns, index\u001b[39m=\u001b[39;49mindex)\n\u001b[0;32m   1797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_currow \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_rows\n\u001b[0;32m   1799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msqueeze \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(df\u001b[39m.\u001b[39mcolumns) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    660\u001b[0m     )\n\u001b[0;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:154\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    151\u001b[0m axes \u001b[39m=\u001b[39m [columns, index]\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mblock\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[39mreturn\u001b[39;00m create_block_manager_from_column_arrays(\n\u001b[0;32m    155\u001b[0m         arrays, axes, consolidate\u001b[39m=\u001b[39;49mconsolidate\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    157\u001b[0m \u001b[39melif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2204\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate)\u001b[0m\n\u001b[0;32m   2202\u001b[0m     \u001b[39mraise\u001b[39;00m construction_error(\u001b[39mlen\u001b[39m(arrays), arrays[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, axes, e)\n\u001b[0;32m   2203\u001b[0m \u001b[39mif\u001b[39;00m consolidate:\n\u001b[1;32m-> 2204\u001b[0m     mgr\u001b[39m.\u001b[39;49m_consolidate_inplace()\n\u001b[0;32m   2205\u001b[0m \u001b[39mreturn\u001b[39;00m mgr\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1871\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1869\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_consolidated():\n\u001b[0;32m   1870\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1871\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m=\u001b[39m _consolidate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks)\n\u001b[0;32m   1872\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1873\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefs \u001b[39m=\u001b[39m _consolidate_with_refs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefs)\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2329\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2327\u001b[0m new_blocks: \u001b[39mlist\u001b[39m[Block] \u001b[39m=\u001b[39m []\n\u001b[0;32m   2328\u001b[0m \u001b[39mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[39min\u001b[39;00m grouper:\n\u001b[1;32m-> 2329\u001b[0m     merged_blocks, _ \u001b[39m=\u001b[39m _merge_blocks(\n\u001b[0;32m   2330\u001b[0m         \u001b[39mlist\u001b[39;49m(group_blocks), dtype\u001b[39m=\u001b[39;49mdtype, can_consolidate\u001b[39m=\u001b[39;49m_can_consolidate\n\u001b[0;32m   2331\u001b[0m     )\n\u001b[0;32m   2332\u001b[0m     new_blocks \u001b[39m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2333\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2392\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2389\u001b[0m     new_mgr_locs \u001b[39m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2391\u001b[0m     bp \u001b[39m=\u001b[39m BlockPlacement(new_mgr_locs)\n\u001b[1;32m-> 2392\u001b[0m     \u001b[39mreturn\u001b[39;00m [new_block_2d(new_values, placement\u001b[39m=\u001b[39;49mbp)], \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2394\u001b[0m \u001b[39m# can't consolidate --> no merge\u001b[39;00m\n\u001b[0;32m   2395\u001b[0m \u001b[39mreturn\u001b[39;00m blocks, \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:2161\u001b[0m, in \u001b[0;36mnew_block_2d\u001b[1;34m(values, placement)\u001b[0m\n\u001b[0;32m   2157\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m ObjectBlock\n\u001b[0;32m   2158\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\n\u001b[1;32m-> 2161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_block_2d\u001b[39m(values: ArrayLike, placement: BlockPlacement):\n\u001b[0;32m   2162\u001b[0m     \u001b[39m# new_block specialized to case with\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m     \u001b[39m#  ndim=2\u001b[39;00m\n\u001b[0;32m   2164\u001b[0m     \u001b[39m#  isinstance(placement, BlockPlacement)\u001b[39;00m\n\u001b[0;32m   2165\u001b[0m     \u001b[39m#  check_ndim/ensure_block_shape already checked\u001b[39;00m\n\u001b[0;32m   2166\u001b[0m     klass \u001b[39m=\u001b[39m get_block_type(values\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   2168\u001b[0m     values \u001b[39m=\u001b[39m maybe_coerce_values(values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/athletes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are Na values in some columns. Fill with 0\n",
    "df['total_lift'] = df['candj'].fillna(0) + df['snatch'].fillna(0) + df['deadlift'].fillna(0) + df['backsq'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vars = ['region','gender', 'eat', 'background', 'experience', 'schedule', 'howlong','age','height','weight','candj','snatch','deadlift','backsq']\n",
    "cats = ['region','gender', 'eat', 'background', 'experience', 'schedule', 'howlong']\n",
    "numcs = ['age','height','weight']\n",
    "\n",
    "# x = pd.get_dummies(x, columns=cats) # encode categorical variables\n",
    "\n",
    "x = df[numcs].fillna(0) # NAs in numeric columns, fill 0 if any\n",
    "\n",
    "y = df['total_lift']\n",
    "\n",
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
    "\n",
    "# model hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Define privacy parameters\n",
    "l2_norm_clip = 1.0  # Clipping norm\n",
    "noise_multiplier = 0.5  # Noise multiplier\n",
    "num_microbatches = 1  # Number of microbatches\n",
    "learning_rate = 0.1  # Learning rate\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(x_train.shape[1],)),   # Input layer for the number of features\n",
    "    tf.keras.layers.Dense(64, activation='relu'),       # Hidden layer 1\n",
    "    tf.keras.layers.Dense(32, activation='relu'),       # Hidden layer 2\n",
    "    tf.keras.layers.Dense(1)                            # Output layer (for regression)\n",
    "])\n",
    "\n",
    "# Create the optimizer with differential privacy\n",
    "optimizer = DPGradientDescentGaussianOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)  # Use `dp_sum_query` for better privacy guarantees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Talha\\OneDrive\\Documents\\Code\\ML-Ops\\code\\modeling_dp.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Talha/OneDrive/Documents/Code/ML-Ops/code/modeling_dp.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mfit(x_train, y_train, epochs\u001b[39m=\u001b[39mepochs, batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy import DPKerasAdamOptimizer\n",
    "\n",
    "# DP parameters\n",
    "l2_norm_clip = 1\n",
    "noise_multiplier = 0.5\n",
    "\n",
    "# model hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(x_train.shape[1],)),   # Input layer for the number of features\n",
    "    tf.keras.layers.Dense(64, activation='relu'),       # Hidden layer 1\n",
    "    tf.keras.layers.Dense(32, activation='relu'),       # Hidden layer 2\n",
    "    tf.keras.layers.Dense(1)                            # Output layer (for regression)\n",
    "])\n",
    "\n",
    "optimizer = DPKerasAdamOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier\n",
    "    )\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError(\n",
    "    reduction=tf.losses.Reduction.NONE,\n",
    "    name='mean_squared_error'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "751/751 [==============================] - 8s 6ms/step - loss: 144811.0312\n",
      "Epoch 2/5\n",
      "751/751 [==============================] - 5s 7ms/step - loss: 48011.8945\n",
      "Epoch 3/5\n",
      "751/751 [==============================] - 5s 6ms/step - loss: 46869.7422\n",
      "Epoch 4/5\n",
      "751/751 [==============================] - 5s 7ms/step - loss: 46250.2852\n",
      "Epoch 5/5\n",
      "751/751 [==============================] - 5s 6ms/step - loss: 45950.0938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a09d3c9810>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=5,\n",
    "          batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 2ms/step - loss: 45189.7461\n",
      "Test loss: 45189.74609375\n",
      "188/188 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss = model.evaluate(x_test, y_test)\n",
    "print(f'Test loss: {test_loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 164.74\n",
      "Mean Squared Error (MSE): 45212.21\n",
      "Root Mean Squared Error (RMSE): 212.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Compute regression metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)  # Calculate RMSE\n",
    "\n",
    "# Display the metrics summary\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute DP epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD performed over 24023 examples with 256 examples per iteration, noise\n",
      "multiplier 0.5 for 10 epochs with microbatching, and no bound on number of\n",
      "examples per user.\n",
      "\n",
      "This privacy guarantee protects the release of all model checkpoints in addition\n",
      "to the final model.\n",
      "\n",
      "Example-level DP with add-or-remove-one adjacency at delta = 1e-05 computed with\n",
      "RDP accounting:\n",
      "    Epsilon with each example occurring once per epoch:       138.688\n",
      "    Epsilon assuming Poisson sampling (*):                    139.347\n",
      "\n",
      "No user-level privacy guarantee is possible without a bound on the number of\n",
      "examples per user.\n",
      "\n",
      "(*) Poisson sampling is not usually done in training pipelines, but assuming\n",
      "that the data was randomly shuffled, it is believed the actual epsilon should be\n",
      "closer to this value than the conservative assumption of an arbitrary data\n",
      "order.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(compute_dp_sgd_privacy.compute_dp_sgd_privacy_statement(x_train.shape[0],\n",
    "                                    batch_size=256,\n",
    "                                    noise_multiplier=noise_multiplier,\n",
    "                                    num_epochs=epochs,\n",
    "                                    delta=.00001\n",
    "                                    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
